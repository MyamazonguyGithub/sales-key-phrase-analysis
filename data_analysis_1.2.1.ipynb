{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c2b4e4",
   "metadata": {},
   "source": [
    "# **Key Phrase Analysis**\n",
    "\n",
    "This notebook performs an analysis to identify key phrases used by account executives in sales transcripts and evaluates their impact on sales outcomes using embeddings and clustering techniques. We'll follow a systematic workflow that includes data loading, text preprocessing, embedding generation, clustering, key phrase extraction, statistical analysis, and interpretation.\n",
    "\n",
    "## **Table of Contents**\n",
    "1. [Introduction](#introduction)\n",
    "2. [Data Loading and Preparation](#data-loading-and-preparation)\n",
    "3. [Text Preprocessing](#text-preprocessing)\n",
    "4. [Embedding Generation](#embedding-generation)\n",
    "5. [Clustering Embeddings](#clustering-embeddings)\n",
    "6. [Key Phrase Extraction](#key-phrase-extraction)\n",
    "7. [Data Aggregation](#data-aggregation)\n",
    "   - 7.1 [Meeting-Level Aggregation](#meeting-level-aggregation)\n",
    "   - 7.2 [Phrase-Level Aggregation with Frequency](#phrase-level-aggregation)\n",
    "8. [Statistical Analysis](#statistical-analysis)\n",
    "   - 8.1 [Meeting-Level Analysis](#meeting-level-analysis)\n",
    "   - 8.2 [Phrase-Level Analysis](#phrase-level-analysis)\n",
    "9. [Model Interpretation](#model-interpretation)\n",
    "10. [Visualization](#visualization)\n",
    "11. [Conclusion](#conclusion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac58bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install necessary packages\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# # Install tqdm\n",
    "# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tqdm\"])\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# packages = [\n",
    "#     \"pandas\", \"matplotlib\", \"seaborn\", \"scipy\",\n",
    "#     \"scikit-learn\", \"sentence_transformers\", \"nltk\", \"umap-learn\",\n",
    "#     \"imblearn\", \"gspread\", \"spacy\", \"numpy<2.0\", \"threadpoolctl\", \"statsmodels\"\n",
    "# ]\n",
    "\n",
    "# # Install packages individually with tqdm progress bar\n",
    "# for package in tqdm(packages, desc=\"Installing packages\"):\n",
    "#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# # Download spaCy model\n",
    "# result = subprocess.run(\"python -m spacy download en_core_web_sm\", shell=True, capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a843a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.stats import chi2_contingency\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import time\n",
    "import warnings\n",
    "import ast  # ðŸ”§âœ¨ Modification: Import ast to handle string to list conversion\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "_ = nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27027a",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## **1. Introduction**\n",
    "\n",
    "The goal of this analysis is to identify key phrases used by account executives in sales transcripts and determine their impact on sales outcomes. By leveraging embeddings and clustering techniques, we aim to uncover phrases that correlate with successful sales (`closed_won`) or longer sales cycles.\n",
    "\n",
    "**Data Structure:**\n",
    "\n",
    "- **IDs List (`ids`):** A list of dictionaries containing transcript IDs and their corresponding sales outcomes.\n",
    "- **Meeting Data (`meeting_df`):** Contains meeting-level information for each transcript (does not include `sales_outcome`).\n",
    "- **Sentences Data (`sentences_df`):** Contains sentence-level data, including speaker information and the text spoken.\n",
    "\n",
    "**Steps Involved:**\n",
    "\n",
    "1. Load and prepare the data from multiple transcripts.\n",
    "2. Preprocess the text data.\n",
    "3. Generate embeddings for sentences spoken by account executives.\n",
    "4. Cluster the embeddings to identify key phrases.\n",
    "5. Extract representative key phrases for each cluster.\n",
    "6. Aggregate data at both meeting and phrase levels.\n",
    "7. Perform statistical analysis to identify significant phrases.\n",
    "8. Interpret the model coefficients and provide insights.\n",
    "9. Visualize the results.\n",
    "10. Draw conclusions and suggest actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae8c77",
   "metadata": {},
   "source": [
    "<a id='data-loading-and-preparation'></a>\n",
    "## **2. Data Loading and Preparation**\n",
    "\n",
    "We'll start by loading the dataset from multiple transcripts and preparing it for analysis.\n",
    "\n",
    "**Note:** We have added an exponential backoff retry mechanism to handle any transient errors during data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4797555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.gspread import (\n",
    "    gspread_try_get_cells_by_range,\n",
    "    gspread_try_get_service_account_from_file,\n",
    "    gspread_try_get_spreadsheet_by_id,\n",
    "    gspread_try_get_worksheet_by_id\n",
    ")\n",
    "\n",
    "# Check if 'all_sentences_df.csv' exists\n",
    "if os.path.exists('all_sentences_df.csv'):\n",
    "    # Read the DataFrame from the CSV file\n",
    "    all_sentences_df = pd.read_csv('all_sentences_df.csv')\n",
    "    print(\"Loaded 'all_sentences_df.csv' successfully.\")\n",
    "else:\n",
    "    # Get data from Google Sheets\n",
    "    service_account = gspread_try_get_service_account_from_file('secret_sales_meetings_report_service_account.json')\n",
    "    spreadsheet = gspread_try_get_spreadsheet_by_id(\n",
    "        service_account,\n",
    "        '19AYpEl2TeqUAAZ-pqGU3c9j6rtzzFbWgRrEjHA_ozYM'  # Production spreadsheet\n",
    "    )\n",
    "    sheet_metrics_per_transcript = gspread_try_get_worksheet_by_id(spreadsheet, 1963712501)\n",
    "\n",
    "    # Fetch IDs from Google Sheets\n",
    "    ids = gspread_try_get_cells_by_range(sheet_metrics_per_transcript, 'A3:E')\n",
    "\n",
    "    # Convert list of dictionaries\n",
    "    ids = [\n",
    "        {\n",
    "            'id': row[0],\n",
    "            'sales_outcome': row[4],\n",
    "        }\n",
    "        for row in ids\n",
    "    ]\n",
    "\n",
    "    from algorithms.get_transcript import get_transcript_dfs\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame and remove duplicates\n",
    "    ids_df = pd.DataFrame(ids)\n",
    "    ids_df = ids_df.drop_duplicates(subset='id')\n",
    "    ids = ids_df.to_dict('records')\n",
    "\n",
    "    # Initialize an empty list to store all sentences\n",
    "    all_sentences = []\n",
    "\n",
    "    # Iterate over the IDs with tqdm for progress tracking\n",
    "    for item in tqdm(ids, desc=\"Processing IDs\"):\n",
    "        transcript_id = item['id']\n",
    "        sales_outcome = item['sales_outcome']\n",
    "\n",
    "        retries = 5\n",
    "        delay = 2  # Initial delay in seconds\n",
    "        for attempt in range(1, retries + 1):\n",
    "            try:\n",
    "                # Fetch the dataframes\n",
    "                transcript_dfs = get_transcript_dfs(transcript_id)\n",
    "                meeting_df = transcript_dfs['meeting_df']\n",
    "                sentences_df = transcript_dfs['sentences_df']\n",
    "\n",
    "                # Add 'sales_outcome' to sentences_df\n",
    "                sentences_df['sales_outcome'] = sales_outcome\n",
    "                sentences_df['id'] = transcript_id\n",
    "\n",
    "                # Filter for account executives\n",
    "                ae_sentences = sentences_df[sentences_df['is_account_executive']]\n",
    "\n",
    "                # Append to list\n",
    "                all_sentences.append(ae_sentences)\n",
    "\n",
    "                # Break out of the retry loop if successful\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt == retries:\n",
    "                    print(f\"Failed to process transcript_id {transcript_id} after {retries} attempts.\")\n",
    "                    raise e  # Re-raise the exception after final attempt\n",
    "                else:\n",
    "                    wait_time = delay * (2 ** (attempt - 1))  # Exponential backoff\n",
    "                    print(f\"Attempt {attempt} failed for transcript_id {transcript_id}. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "\n",
    "    # Concatenate all sentences into a single DataFrame\n",
    "    all_sentences_df = pd.concat(all_sentences, ignore_index=True)\n",
    "\n",
    "    # Display the first few rows\n",
    "    print(all_sentences_df.head())\n",
    "\n",
    "    # Export the DataFrame to CSV\n",
    "    all_sentences_df.to_csv('all_sentences_df.csv', index=False)\n",
    "    print(\"Saved 'all_sentences_df.csv' successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6dd3d",
   "metadata": {},
   "source": [
    "<a id='text-preprocessing'></a>\n",
    "## **3. Text Preprocessing**\n",
    "\n",
    "We'll clean and prepare the text data for embedding generation, focusing on extracting meaningful content by retaining only nouns and adjectives. We'll set up the text preprocessing procedure to break down each sentence into individual phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = stop_words.union({'yeah', 'um', 'so', 'like', 'uh', 'hmm'})\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text_spacy_adj_nouns(text):\n",
    "    # Check for NaN\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    \n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    # Initialize list to hold meaningful phrases\n",
    "    phrases = []\n",
    "    \n",
    "    # Extract noun chunks (noun phrases)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        # Collect adjectives and nouns within the chunk\n",
    "        tokens = [\n",
    "            token.text\n",
    "            for token in chunk\n",
    "            if token.pos_ in ['ADJ', 'NOUN', 'PROPN'] and token.is_alpha and not token.is_stop and token.text not in custom_stop_words\n",
    "        ]\n",
    "        if tokens:\n",
    "            phrase = ' '.join(tokens)\n",
    "            phrases.append(phrase)\n",
    "    \n",
    "    # Extract named entities\n",
    "    entities = [\n",
    "        ent.text.lower()\n",
    "        for ent in doc.ents\n",
    "        if ent.label_ in ['PERSON', 'ORG', 'GPE', 'PRODUCT', 'EVENT']\n",
    "    ]\n",
    "    phrases.extend(entities)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_phrases = []\n",
    "    for phrase in phrases:\n",
    "        if phrase not in seen:\n",
    "            seen.add(phrase)\n",
    "            unique_phrases.append(phrase)\n",
    "    \n",
    "    return unique_phrases\n",
    "\n",
    "# Enable the tqdm progress bar for pandas apply\n",
    "tqdm.pandas()\n",
    "\n",
    "# ðŸ”§âœ¨ Modification: Use 'ast.literal_eval' to convert 'phrases' column back to lists\n",
    "# Check if 'all_sentences_df_with_phrases.csv' exists\n",
    "if os.path.exists('all_sentences_df_with_phrases.csv'):\n",
    "    # Load the DataFrame with phrases\n",
    "    all_sentences_df = pd.read_csv('all_sentences_df_with_phrases.csv')\n",
    "    print(\"Loaded 'all_sentences_df_with_phrases.csv' successfully.\")\n",
    "    \n",
    "    # Convert 'phrases' column from string representation to actual lists\n",
    "    all_sentences_df['phrases'] = all_sentences_df['phrases'].progress_apply(ast.literal_eval)\n",
    "else:\n",
    "    # Check if 'all_sentences_df.csv' exists\n",
    "    if os.path.exists('all_sentences_df.csv'):\n",
    "        # Load the DataFrame without phrases\n",
    "        all_sentences_df = pd.read_csv('all_sentences_df.csv')\n",
    "        print(\"Loaded 'all_sentences_df.csv' successfully.\")\n",
    "    else:\n",
    "        # Get data from Google Sheets\n",
    "        service_account = gspread_try_get_service_account_from_file('secret_sales_meetings_report_service_account.json')\n",
    "        spreadsheet = gspread_try_get_spreadsheet_by_id(\n",
    "            service_account,\n",
    "            '19AYpEl2TeqUAAZ-pqGU3c9j6rtzzFbWgRrEjHA_ozYM'  # Production spreadsheet\n",
    "        )\n",
    "        sheet_metrics_per_transcript = gspread_try_get_worksheet_by_id(spreadsheet, 1963712501)\n",
    "    \n",
    "        # Fetch IDs from Google Sheets\n",
    "        ids = gspread_try_get_cells_by_range(sheet_metrics_per_transcript, 'A3:E')\n",
    "    \n",
    "        # Convert list of dictionaries\n",
    "        ids = [\n",
    "            {\n",
    "                'id': row[0],\n",
    "                'sales_outcome': row[4],\n",
    "            }\n",
    "            for row in ids\n",
    "        ]\n",
    "    \n",
    "        # Convert the list of dictionaries to a DataFrame and remove duplicates\n",
    "        ids_df = pd.DataFrame(ids)\n",
    "        ids_df = ids_df.drop_duplicates(subset='id')\n",
    "        ids = ids_df.to_dict('records')\n",
    "    \n",
    "        # Initialize an empty list to store all sentences\n",
    "        all_sentences = []\n",
    "    \n",
    "        # Iterate over the IDs with tqdm for progress tracking\n",
    "        for item in tqdm(ids, desc=\"Processing IDs\"):\n",
    "            transcript_id = item['id']\n",
    "            sales_outcome = item['sales_outcome']\n",
    "    \n",
    "            retries = 5\n",
    "            delay = 2  # Initial delay in seconds\n",
    "            for attempt in range(1, retries + 1):\n",
    "                try:\n",
    "                    # Fetch the dataframes\n",
    "                    transcript_dfs = get_transcript_dfs(transcript_id)\n",
    "                    meeting_df = transcript_dfs['meeting_df']\n",
    "                    sentences_df = transcript_dfs['sentences_df']\n",
    "    \n",
    "                    # Add 'sales_outcome' to sentences_df\n",
    "                    sentences_df['sales_outcome'] = sales_outcome\n",
    "                    sentences_df['id'] = transcript_id\n",
    "    \n",
    "                    # Filter for account executives\n",
    "                    ae_sentences = sentences_df[sentences_df['is_account_executive']]\n",
    "    \n",
    "                    # Append to list\n",
    "                    all_sentences.append(ae_sentences)\n",
    "    \n",
    "                    # Break out of the retry loop if successful\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt == retries:\n",
    "                        print(f\"Failed to process transcript_id {transcript_id} after {retries} attempts.\")\n",
    "                        raise e  # Re-raise the exception after final attempt\n",
    "                    else:\n",
    "                        wait_time = delay * (2 ** (attempt - 1))  # Exponential backoff\n",
    "                        print(f\"Attempt {attempt} failed for transcript_id {transcript_id}. Retrying in {wait_time} seconds...\")\n",
    "                        time.sleep(wait_time)\n",
    "    \n",
    "        # Concatenate all sentences into a single DataFrame\n",
    "        all_sentences_df = pd.concat(all_sentences, ignore_index=True)\n",
    "    \n",
    "        # Display the first few rows\n",
    "        print(all_sentences_df.head())\n",
    "    \n",
    "        # Export the DataFrame to CSV\n",
    "        all_sentences_df.to_csv('all_sentences_df.csv', index=False)\n",
    "        print(\"Saved 'all_sentences_df.csv' successfully.\")\n",
    "\n",
    "        # Assign the DataFrame to proceed with phrases extraction\n",
    "        all_sentences_df = all_sentences_df\n",
    "\n",
    "    # Apply the updated preprocessing function\n",
    "    all_sentences_df['phrases'] = all_sentences_df['text'].progress_apply(preprocess_text_spacy_adj_nouns)\n",
    "    \n",
    "    # Display the phrases\n",
    "    print(all_sentences_df[['text', 'phrases']].head())\n",
    "    \n",
    "    # Export the DataFrame to CSV with phrases\n",
    "    all_sentences_df.to_csv('all_sentences_df_with_phrases.csv', index=False)\n",
    "    print(\"Saved 'all_sentences_df_with_phrases.csv' successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f548f97",
   "metadata": {},
   "source": [
    "### **Explode Phrases Into Separate Rows**\n",
    "\n",
    "We will expand the DataFrame so that each phrase has its own row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§âœ¨ Modification: Ensure 'phrases' column contains lists before exploding\n",
    "# Check if 'expanded_df.parquet' exists\n",
    "if os.path.exists('expanded_df.parquet'):\n",
    "    # Load the DataFrame from the Parquet file\n",
    "    expanded_df = pd.read_parquet('expanded_df.parquet')\n",
    "    print(\"Loaded 'expanded_df.parquet' successfully.\")\n",
    "else:\n",
    "    # Ensure 'phrases' column contains lists, not strings\n",
    "    if all_sentences_df['phrases'].apply(lambda x: isinstance(x, list)).all():\n",
    "        print(\"Confirmed that 'phrases' column contains lists.\")\n",
    "    else:\n",
    "        print(\"Converting 'phrases' column to lists using ast.literal_eval.\")\n",
    "        all_sentences_df['phrases'] = all_sentences_df['phrases'].apply(ast.literal_eval)\n",
    "    \n",
    "    # Explode the phrases into separate rows\n",
    "    expanded_df = all_sentences_df.explode('phrases').reset_index(drop=True)\n",
    "    \n",
    "    # Rename 'phrases' to 'clean_text' for consistency\n",
    "    expanded_df.rename(columns={'phrases': 'clean_text'}, inplace=True)\n",
    "    \n",
    "    # Drop rows where 'clean_text' is NaN or empty\n",
    "    expanded_df = expanded_df[expanded_df['clean_text'].notna() & (expanded_df['clean_text'] != '')]\n",
    "    \n",
    "    # Display the expanded DataFrame\n",
    "    print(expanded_df.head())\n",
    "    \n",
    "    # Save the DataFrame to a Parquet file\n",
    "    expanded_df.to_parquet('expanded_df.parquet', index=False)\n",
    "    print(\"Saved 'expanded_df.parquet' successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43789c2e",
   "metadata": {},
   "source": [
    "<a id='embedding-generation'></a>\n",
    "## **4. Embedding Generation**\n",
    "\n",
    "We'll generate embeddings for each phrase using a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'expanded_df_with_embeddings.parquet' exists\n",
    "if os.path.exists('expanded_df_with_embeddings.parquet'):\n",
    "    # Load the DataFrame with embeddings from Parquet\n",
    "    expanded_df = pd.read_parquet('expanded_df_with_embeddings.parquet')\n",
    "    print(\"Loaded 'expanded_df_with_embeddings.parquet' successfully.\")\n",
    "else:\n",
    "    # Load the model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generate embeddings using the individual phrases\n",
    "    embeddings = model.encode(expanded_df['clean_text'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    # Add embeddings to DataFrame\n",
    "    expanded_df['embedding'] = embeddings.tolist()\n",
    "    \n",
    "    # Display the first few embeddings\n",
    "    expanded_df.head()\n",
    "    \n",
    "    # Save the expanded DataFrame to a Parquet file\n",
    "    expanded_df.to_parquet('expanded_df_with_embeddings.parquet', index=False)\n",
    "    print(\"Saved 'expanded_df_with_embeddings.parquet' successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6060dff",
   "metadata": {},
   "source": [
    "<a id='clustering-embeddings'></a>\n",
    "## **5. Clustering Embeddings**\n",
    "\n",
    "We'll cluster the phrase embeddings to identify groups of similar phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f6b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Set environment variables to limit thread usage\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# Check if 'expanded_df_with_clusters.parquet' exists\n",
    "if os.path.exists('expanded_df_with_clusters.parquet'):\n",
    "    # Load the DataFrame from the Parquet file\n",
    "    expanded_df = pd.read_parquet('expanded_df_with_clusters.parquet')\n",
    "    print(\"Loaded 'expanded_df_with_clusters.parquet' successfully.\")\n",
    "else:\n",
    "    # Ensure 'expanded_df_with_embeddings.parquet' is available\n",
    "    if not os.path.exists('expanded_df_with_embeddings.parquet'):\n",
    "        raise FileNotFoundError(\"expanded_df_with_embeddings.parquet not found. Please generate it first.\")\n",
    "    \n",
    "    # Load the embeddings DataFrame\n",
    "    expanded_df = pd.read_parquet('expanded_df_with_embeddings.parquet')\n",
    "    print(\"Loaded 'expanded_df_with_embeddings.parquet' successfully.\")\n",
    "    \n",
    "    # Convert embeddings to float32 to save memory\n",
    "    try:\n",
    "        embedding_array = np.array(expanded_df['embedding'].tolist(), dtype=np.float32)\n",
    "    except MemoryError:\n",
    "        print(\"MemoryError: Unable to convert embeddings to a NumPy array with float32.\")\n",
    "        print(\"Consider reducing the dataset size or using incremental processing.\")\n",
    "        raise\n",
    "    \n",
    "    # Dimensionality Reduction using PCA (Incremental)\n",
    "    print(\"Starting PCA dimensionality reduction...\")\n",
    "    pca = PCA(n_components=0.95, random_state=42)\n",
    "    \n",
    "    # Fit PCA on the entire dataset (if memory allows)\n",
    "    try:\n",
    "        reduced_embeddings = pca.fit_transform(embedding_array)\n",
    "    except MemoryError:\n",
    "        print(\"MemoryError: PCA transformation failed due to insufficient memory.\")\n",
    "        print(\"Consider using IncrementalPCA or reducing the dataset size.\")\n",
    "        raise\n",
    "    \n",
    "    print(\"PCA completed successfully.\")\n",
    "    \n",
    "    # Clustering using MiniBatchKMeans\n",
    "    print(\"Starting MiniBatchKMeans clustering...\")\n",
    "    num_clusters = 100  # Adjust as needed\n",
    "    \n",
    "    kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=42, batch_size=10000)\n",
    "    try:\n",
    "        kmeans.fit(reduced_embeddings)\n",
    "    except MemoryError:\n",
    "        print(\"MemoryError: MiniBatchKMeans clustering failed due to insufficient memory.\")\n",
    "        print(\"Consider reducing the number of clusters or the batch size.\")\n",
    "        raise\n",
    "    \n",
    "    print(\"Clustering completed successfully.\")\n",
    "    \n",
    "    # Step 7: Assign cluster labels\n",
    "    expanded_df['cluster'] = kmeans.labels_\n",
    "    \n",
    "    # Step 8: Display cluster assignments\n",
    "    print(\"Cluster assignments:\")\n",
    "    print(expanded_df[['id', 'cluster']].head())\n",
    "    \n",
    "    # Step 9: Export the DataFrame with cluster assignments to Parquet\n",
    "    expanded_df.to_parquet('expanded_df_with_clusters.parquet', index=False)\n",
    "    print(\"Saved 'expanded_df_with_clusters.parquet' successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27577785",
   "metadata": {},
   "source": [
    "<a id='key-phrase-extraction'></a>\n",
    "## **6. Key Phrase Extraction**\n",
    "\n",
    "We'll extract representative key phrases for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'expanded_df_with_key_phrases.parquet' exists\n",
    "if os.path.exists('expanded_df_with_key_phrases.parquet'):\n",
    "    # Load the DataFrame from the Parquet file\n",
    "    expanded_df = pd.read_parquet('expanded_df_with_key_phrases.parquet')\n",
    "    print(\"Loaded 'expanded_df_with_key_phrases.parquet' successfully.\")\n",
    "else:\n",
    "    # Ensure 'expanded_df' is available\n",
    "    # If not, load it from the previous Parquet file\n",
    "    if 'expanded_df' not in locals():\n",
    "        if os.path.exists('expanded_df_with_clusters.parquet'):\n",
    "            expanded_df = pd.read_parquet('expanded_df_with_clusters.parquet')\n",
    "            print(\"Loaded 'expanded_df_with_clusters.parquet' successfully.\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"expanded_df_with_clusters.parquet not found. Please generate it first.\")\n",
    "\n",
    "    # Ensure 'num_clusters' is defined\n",
    "    if 'num_clusters' not in locals():\n",
    "        num_clusters = expanded_df['cluster'].nunique()\n",
    "\n",
    "    # Initialize a dictionary to store cluster labels\n",
    "    cluster_labels = {}\n",
    "\n",
    "    for cluster_num in range(num_clusters):\n",
    "        # Filter phrases in the cluster\n",
    "        cluster_phrases = expanded_df[expanded_df['cluster'] == cluster_num]\n",
    "\n",
    "        if cluster_phrases.empty:\n",
    "            # Assign 'N/A' if the cluster has no phrases\n",
    "            key_phrase = 'N/A'\n",
    "            print(f\"Cluster {cluster_num} has no phrases. Assigned key phrase: {key_phrase}\\n\")\n",
    "        else:\n",
    "            # Get all phrases in the cluster\n",
    "            phrases = cluster_phrases['clean_text'].tolist()\n",
    "\n",
    "            # Count the frequency of each phrase\n",
    "            phrase_counts = pd.Series(phrases).value_counts()\n",
    "\n",
    "            # Assign the most frequent phrase as the key phrase\n",
    "            key_phrase = phrase_counts.index[0]\n",
    "            print(f\"Cluster {cluster_num} assigned key phrase: {key_phrase}\\n\")\n",
    "\n",
    "        # Store the assigned key phrase\n",
    "        cluster_labels[cluster_num] = key_phrase\n",
    "\n",
    "    # Map cluster labels\n",
    "    expanded_df['key_phrase'] = expanded_df['cluster'].map(cluster_labels)\n",
    "\n",
    "    # Display the DataFrame with key phrases\n",
    "    print(expanded_df[['clean_text', 'cluster', 'key_phrase']].head())\n",
    "\n",
    "    # Export the DataFrame with key phrases to Parquet\n",
    "    expanded_df.to_parquet('expanded_df_with_key_phrases.parquet', index=False)\n",
    "    print(\"Saved 'expanded_df_with_key_phrases.parquet' successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ae935",
   "metadata": {},
   "source": [
    "<a id='data-aggregation'></a>\n",
    "## **7. Data Aggregation**\n",
    "\n",
    "We'll perform two types of data aggregation:\n",
    "\n",
    "### **7.1 Meeting-Level Aggregation**\n",
    "\n",
    "Here, we aggregate key phrases at the meeting level, indicating whether each key phrase was used in the meeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2476c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'meeting_level_df.parquet' exists\n",
    "if os.path.exists('meeting_level_df.parquet'):\n",
    "    # Load the DataFrame from the Parquet file\n",
    "    meeting_level_df = pd.read_parquet('meeting_level_df.parquet')\n",
    "    print(\"Loaded 'meeting_level_df.parquet' successfully.\")\n",
    "else:\n",
    "    # Ensure 'expanded_df' is available\n",
    "    if 'expanded_df' not in locals():\n",
    "        # Try to load 'expanded_df_with_key_phrases.parquet'\n",
    "        if os.path.exists('expanded_df_with_key_phrases.parquet'):\n",
    "            expanded_df = pd.read_parquet('expanded_df_with_key_phrases.parquet')\n",
    "            print(\"Loaded 'expanded_df_with_key_phrases.parquet' successfully.\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"expanded_df_with_key_phrases.parquet not found. Please generate it first.\")\n",
    "\n",
    "    # Group by meeting ID and collect unique key phrases\n",
    "    meeting_key_phrases = expanded_df.groupby('id')['key_phrase'].apply(lambda x: list(set(x))).reset_index()\n",
    "\n",
    "    # Get the sales outcome for each meeting\n",
    "    meeting_outcomes = expanded_df[['id', 'sales_outcome']].drop_duplicates()\n",
    "\n",
    "    # Merge key phrases with sales outcomes\n",
    "    meeting_level_df = pd.merge(meeting_key_phrases, meeting_outcomes, on='id')\n",
    "\n",
    "    # Create binary features for each key phrase\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    key_phrase_matrix = mlb.fit_transform(meeting_level_df['key_phrase'])\n",
    "    key_phrase_df = pd.DataFrame(key_phrase_matrix, columns=mlb.classes_)\n",
    "    meeting_level_df = pd.concat([meeting_level_df, key_phrase_df], axis=1)\n",
    "\n",
    "    # Map sales outcome to binary\n",
    "    meeting_level_df['sales_outcome_binary'] = meeting_level_df['sales_outcome'].map({'closed_won': 1, 'closed_lost': 0})\n",
    "\n",
    "    # Display the meeting-level DataFrame\n",
    "    print(meeting_level_df.head())\n",
    "\n",
    "    # Export the DataFrame to Parquet\n",
    "    meeting_level_df.to_parquet('meeting_level_df.parquet', index=False)\n",
    "    print(\"Saved 'meeting_level_df.parquet' successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1cfac4",
   "metadata": {},
   "source": [
    "**Implications:**\n",
    "\n",
    "- **Pros:** Captures whether a key phrase was mentioned in a meeting, linking it to the overall sales outcome.\n",
    "- **Cons:** Does not consider how frequently a phrase was mentioned, potentially overlooking the impact of repetition.\n",
    "\n",
    "### **7.2 Phrase-Level Aggregation with Frequency**\n",
    "\n",
    "In this approach, we consider the frequency of each key phrase within each meeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5328e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'phrase_level_df.parquet' exists\n",
    "if os.path.exists('phrase_level_df.parquet'):\n",
    "    # Load the DataFrame from the Parquet file\n",
    "    phrase_level_df = pd.read_parquet('phrase_level_df.parquet')\n",
    "    print(\"Loaded 'phrase_level_df.parquet' successfully.\")\n",
    "else:\n",
    "    # Ensure 'expanded_df' is available\n",
    "    if 'expanded_df' not in locals():\n",
    "        # Try to load 'expanded_df_with_key_phrases.parquet'\n",
    "        if os.path.exists('expanded_df_with_key_phrases.parquet'):\n",
    "            expanded_df = pd.read_parquet('expanded_df_with_key_phrases.parquet')\n",
    "            print(\"Loaded 'expanded_df_with_key_phrases.parquet' successfully.\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"expanded_df_with_key_phrases.parquet not found. Please generate it first.\")\n",
    "\n",
    "    # Phrase-Level Aggregation with Frequency\n",
    "\n",
    "    # Count the frequency of each key phrase per meeting\n",
    "    phrase_frequency = expanded_df.groupby(['id', 'key_phrase']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "    # Get the sales outcome for each meeting\n",
    "    meeting_outcomes = expanded_df[['id', 'sales_outcome']].drop_duplicates()\n",
    "\n",
    "    # Merge phrase frequencies with sales outcomes\n",
    "    phrase_level_df = pd.merge(phrase_frequency, meeting_outcomes, on='id')\n",
    "\n",
    "    # Map sales outcome to binary\n",
    "    phrase_level_df['sales_outcome_binary'] = phrase_level_df['sales_outcome'].map({'closed_won': 1, 'closed_lost': 0})\n",
    "\n",
    "    # Display the phrase-level DataFrame\n",
    "    print(phrase_level_df.head())\n",
    "\n",
    "    # Export the DataFrame to Parquet\n",
    "    phrase_level_df.to_parquet('phrase_level_df.parquet', index=False)\n",
    "    print(\"Saved 'phrase_level_df.parquet' successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f30f7",
   "metadata": {},
   "source": [
    "**Implications:**\n",
    "\n",
    "- **Pros:** Accounts for how often a key phrase is mentioned in a meeting, which may influence the sales outcome.\n",
    "- **Cons:** Increases data complexity and may require different statistical models to account for count data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a68e1b",
   "metadata": {},
   "source": [
    "<a id='statistical-analysis'></a>\n",
    "## **8. Statistical Analysis**\n",
    "\n",
    "We'll perform statistical analysis on both datasets.\n",
    "\n",
    "### **8.1 Meeting-Level Analysis**\n",
    "\n",
    "#### **Chi-Square Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'significant_phrases_meeting.csv' exists\n",
    "if os.path.exists('significant_phrases_meeting.csv'):\n",
    "    # Load the DataFrame from the CSV file\n",
    "    significant_df_meeting = pd.read_csv('significant_phrases_meeting.csv')\n",
    "    print(\"Loaded 'significant_phrases_meeting.csv' successfully.\")\n",
    "else:\n",
    "    # Ensure 'meeting_level_df' is available\n",
    "    if 'meeting_level_df' not in locals():\n",
    "        # Try to load 'meeting_level_df.csv'\n",
    "        if os.path.exists('meeting_level_df.csv'):\n",
    "            meeting_level_df = pd.read_csv('meeting_level_df.csv')\n",
    "            print(\"Loaded 'meeting_level_df.csv' successfully.\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"meeting_level_df.csv not found. Please generate it first.\")\n",
    "\n",
    "    # Ensure 'mlb' (MultiLabelBinarizer) is available\n",
    "    if 'mlb' not in locals():\n",
    "        from sklearn.preprocessing import MultiLabelBinarizer\n",
    "        # Assuming 'key_phrase' column exists in meeting_level_df\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        # Fit the mlb on the 'key_phrase' column\n",
    "        mlb.fit(meeting_level_df['key_phrase'])\n",
    "\n",
    "    # Perform chi-square tests for each key phrase\n",
    "    significant_phrases_meeting = []\n",
    "\n",
    "    for phrase in mlb.classes_:\n",
    "        if phrase in meeting_level_df.columns:\n",
    "            contingency_table = pd.crosstab(meeting_level_df[phrase], meeting_level_df['sales_outcome_binary'])\n",
    "            if contingency_table.shape == (2, 2):\n",
    "                chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "                if p < 0.05:\n",
    "                    significant_phrases_meeting.append({'phrase': phrase, 'chi2': chi2, 'p_value': p})\n",
    "        else:\n",
    "            print(f\"Phrase '{phrase}' not found in meeting_level_df columns.\")\n",
    "\n",
    "    significant_df_meeting = pd.DataFrame(significant_phrases_meeting)\n",
    "\n",
    "    print(\"Significant phrases based on chi-square tests (Meeting Level):\")\n",
    "    print(significant_df_meeting)\n",
    "\n",
    "    # Export to CSV\n",
    "    significant_df_meeting.to_csv('significant_phrases_meeting.csv', index=False)\n",
    "    print(\"Saved 'significant_phrases_meeting.csv' successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9eb6f0",
   "metadata": {},
   "source": [
    "#### **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = meeting_level_df[mlb.classes_]\n",
    "y = meeting_level_df['sales_outcome_binary']\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution in y:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Apply SMOTE to Address Class Imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check new class distribution\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report (Meeting Level):\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae301a1",
   "metadata": {},
   "source": [
    "### **8.2 Phrase-Level Analysis**\n",
    "\n",
    "Since we're dealing with count data, we'll use a different approach.\n",
    "\n",
    "#### **Poisson Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e025dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_counts = phrase_level_df.drop(columns=['id', 'sales_outcome', 'sales_outcome_binary'])\n",
    "y_counts = phrase_level_df['sales_outcome_binary']\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X_counts = sm.add_constant(X_counts)\n",
    "\n",
    "# Build the Poisson regression model\n",
    "poisson_model = sm.GLM(y_counts, X_counts, family=sm.families.Poisson()).fit()\n",
    "\n",
    "# Display the summary\n",
    "print(poisson_model.summary())\n",
    "\n",
    "# Export the model summary to a text file\n",
    "with open('poisson_model_summary.txt', 'w') as f:\n",
    "    f.write(poisson_model.summary().as_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca3168",
   "metadata": {},
   "source": [
    "<a id='model-interpretation'></a>\n",
    "## **9. Model Interpretation**\n",
    "\n",
    "### **Meeting-Level Model Interpretation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f989c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'coefficients_meeting.csv' exists\n",
    "if os.path.exists('coefficients_meeting.csv'):\n",
    "    # Load the coefficients DataFrame from the CSV file\n",
    "    coefficients_meeting = pd.read_csv('coefficients_meeting.csv')\n",
    "    print(\"Loaded 'coefficients_meeting.csv' successfully.\")\n",
    "\n",
    "    # Display the top phrases\n",
    "    print(\"Top phrases influencing sales outcome (Meeting Level):\")\n",
    "    print(coefficients_meeting.head(10))\n",
    "else:\n",
    "    # Ensure 'X' and 'model' are available\n",
    "    if 'X' not in locals() or 'model' not in locals():\n",
    "        # Try to load 'meeting_level_df.csv' and re-fit the model\n",
    "        if os.path.exists('meeting_level_df.csv'):\n",
    "            meeting_level_df = pd.read_csv('meeting_level_df.csv')\n",
    "            print(\"Loaded 'meeting_level_df.csv' successfully.\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"meeting_level_df.csv not found. Please generate it first.\")\n",
    "\n",
    "        # Prepare the data for modeling\n",
    "        # Exclude non-feature columns\n",
    "        feature_columns = meeting_level_df.columns.difference(['id', 'sales_outcome', 'sales_outcome_binary'])\n",
    "        X = meeting_level_df[feature_columns]\n",
    "        y = meeting_level_df['sales_outcome_binary']\n",
    "\n",
    "        # Import logistic regression model\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "        # Fit the logistic regression model\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        print(\"Logistic regression model fitted successfully.\")\n",
    "\n",
    "    # Extract coefficients\n",
    "    coefficients_meeting = pd.DataFrame({\n",
    "        'phrase': X.columns,\n",
    "        'coefficient': model.coef_[0]\n",
    "    })\n",
    "\n",
    "    # Calculate odds ratios\n",
    "    coefficients_meeting['odds_ratio'] = np.exp(coefficients_meeting['coefficient'])\n",
    "\n",
    "    # Sort by absolute value of coefficients\n",
    "    coefficients_meeting['abs_coefficient'] = coefficients_meeting['coefficient'].abs()\n",
    "    coefficients_meeting.sort_values('abs_coefficient', ascending=False, inplace=True)\n",
    "\n",
    "    # Display the top phrases\n",
    "    print(\"Top phrases influencing sales outcome (Meeting Level):\")\n",
    "    print(coefficients_meeting.head(10))\n",
    "\n",
    "    # Export to CSV\n",
    "    coefficients_meeting.to_csv('coefficients_meeting.csv', index=False)\n",
    "    print(\"Saved 'coefficients_meeting.csv' successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76d4d8",
   "metadata": {},
   "source": [
    "### **Phrase-Level Model Interpretation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44edbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract significant phrases from Poisson regression\n",
    "poisson_results = poisson_model.summary2().tables[1]\n",
    "significant_phrases_phrase = poisson_results[poisson_results['P>|z|'] < 0.05]\n",
    "significant_phrases_phrase.reset_index(inplace=True)\n",
    "significant_phrases_phrase.rename(columns={'index': 'phrase'}, inplace=True)\n",
    "\n",
    "print(\"Significant phrases based on Poisson regression (Phrase Level):\")\n",
    "display(significant_phrases_phrase[['phrase', 'Coef.', 'P>|z|']])\n",
    "\n",
    "# Export to CSV\n",
    "significant_phrases_phrase.to_csv('significant_phrases_phrase.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf89e88",
   "metadata": {},
   "source": [
    "<a id='visualization'></a>\n",
    "## **10. Visualization**\n",
    "\n",
    "### **Meeting-Level Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f11786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate positive and negative coefficients\n",
    "top_positive_meeting = coefficients_meeting[coefficients_meeting['coefficient'] > 0].head(100)\n",
    "top_negative_meeting = coefficients_meeting[coefficients_meeting['coefficient'] < 0].head(100)\n",
    "\n",
    "# Plotting top positive phrases\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='coefficient', y='phrase', data=top_positive_meeting, color='green')\n",
    "plt.title('Top Positive Key Phrases (Meeting Level)')\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Key Phrase')\n",
    "plt.show()\n",
    "\n",
    "# Plotting top negative phrases\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='coefficient', y='phrase', data=top_negative_meeting, color='red')\n",
    "plt.title('Top Negative Key Phrases (Meeting Level)')\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Key Phrase')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e4e9d2",
   "metadata": {},
   "source": [
    "### **Phrase-Level Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Poisson regression results\n",
    "# Extract top positive and negative coefficients\n",
    "significant_phrases_phrase['abs_coef'] = significant_phrases_phrase['Coef.'].abs()\n",
    "significant_phrases_phrase.sort_values('abs_coef', ascending=False, inplace=True)\n",
    "\n",
    "top_positive_phrase = significant_phrases_phrase[significant_phrases_phrase['Coef.'] > 0].head(100)\n",
    "top_negative_phrase = significant_phrases_phrase[significant_phrases_phrase['Coef.'] < 0].head(100)\n",
    "\n",
    "# Plotting top positive phrases\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='Coef.', y='phrase', data=top_positive_phrase, color='green')\n",
    "plt.title('Top Positive Key Phrases (Phrase Level)')\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Key Phrase')\n",
    "plt.show()\n",
    "\n",
    "# Plotting top negative phrases\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='Coef.', y='phrase', data=top_negative_phrase, color='red')\n",
    "plt.title('Top Negative Key Phrases (Phrase Level)')\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Key Phrase')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9890a927",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## **11. Conclusion**\n",
    "\n",
    "TBA Subtask 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
